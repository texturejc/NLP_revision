{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108c5f33-039f-4474-b9ae-ba61671f3d05",
   "metadata": {},
   "source": [
    "# 1. Edit distances and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08968e8-4bbb-498e-b562-9cd295e51d3f",
   "metadata": {},
   "source": [
    "## Levensthein distance\n",
    "\n",
    "Any string can be turned into another string by inserting, deleting, or substituting characters. The Levenshtein distance between two strings counts the minimum number of edits needed to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b0249b-460e-4739-a745-94b6a8ac2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('words')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import words\n",
    "from random import sample\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b026e8cf-7bfa-4db8-8601-821188a8e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = words.words()\n",
    "\n",
    "samples = sample(word_list, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e898e8f6-cc19-4af4-8234-36f6ace49567",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [] #Calculates edit distances\n",
    "\n",
    "for i in samples:\n",
    "    dist = []\n",
    "    for j in samples:\n",
    "        dist.append(edit_distance(i, j))\n",
    "    distances.append(dist)\n",
    "\n",
    "df = pd.DataFrame(distances)\n",
    "df.columns = samples\n",
    "df.index = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c875e-c2b1-4de8-bd5d-f6a4c661b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b67fe-ae69-4651-9b44-a819bd0f3cb3",
   "metadata": {},
   "source": [
    "## Jaccard distance\n",
    "\n",
    "Jaccard distance derives from a related measure called Jaccard similarity. Jaccard similarity measures how similar two sets are to one another. It is defined as the intersection of the two sets divided by their union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c9f4f-3c85-4276-b25b-87066798cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"The world is all that is the case\"\n",
    "text_2 = \"The sun shone, having no alternative, on the nothing new\"\n",
    "text_3 = \"It was a bright cold day in April, and the clocks were striking thirteen\"\n",
    "\n",
    "texts = [set(text_1.split(\" \")), set(text_2.split(\" \")), set(text_3.split(\" \"))]\n",
    "\n",
    "j_distances = []\n",
    "\n",
    "for i in texts:\n",
    "    dist = []\n",
    "    for j in texts:\n",
    "        dist.append(jaccard_distance(i, j))\n",
    "    j_distances.append(dist)\n",
    "\n",
    "df = pd.DataFrame(j_distances)\n",
    "df.columns = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "df.index = [\"Text 1\", \"Text 2\", \"Text 3\"]\n",
    "\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55261878-3981-4bc3-935f-9d2b3299111f",
   "metadata": {},
   "source": [
    "# 2. Stems and Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4e893-ea32-416b-a15f-4d5452f4c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc2d4cd5-4cb3-4396-8145-be832cae99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"The leaves on the ground started to decompose as they were left \\\n",
    "        untouched, and the leaf blower remained unused in the shed.\"\n",
    "\n",
    "words = word_tokenize(sent)\n",
    "stems = [stemmer.stem(i) for i in words]\n",
    "lemmas = [lemmatizer.lemmatize(i) for i in words if i not in punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d933165-6d16-47c9-985f-00fe82509dc3",
   "metadata": {},
   "source": [
    "# 3. APIs for data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa3e0891-fee0-45e7-9619-61fab4ae2af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your password:  ········\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import datetime\n",
    "from getpass import getpass\n",
    "password = getpass(\"Enter your password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9c8ac5d-dac8-4723-866b-361a5f4be8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(user_agent='VAD',\n",
    "                     client_id='eCo_TWE_BA_zFA', client_secret=\"1gsqXgMrZBoQBVYf40hgtvMS_Ro\",\n",
    "                     username='textureai', password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d36980c-ae60-48f1-bdf8-83f53b420747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(submission_id): ## submission_id can be URL or submission ID\n",
    "    try:\n",
    "        submission = reddit.submission(url = submission_id)\n",
    "    except:\n",
    "        submission = reddit.submission(submission_id)\n",
    "    title = submission.title\n",
    "    submission.comments.replace_more() ## loads new page if cooments are multipage\n",
    "    text = [i.body for i in submission.comments]\n",
    "    score = [i.score for i in submission.comments]\n",
    "    user = [i.author for i in submission.comments]\n",
    "    date = [datetime.datetime.fromtimestamp(i.created) for i in submission.comments]\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = text\n",
    "    df['datetime'] = date\n",
    "    df['score'] = score\n",
    "    df['subreddit'] = submission.subreddit\n",
    "    df['redditor'] = user\n",
    "    df['type'] = 'comment'\n",
    "    df['title'] = title\n",
    "    df = df.sort_values('score', ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f18b013-1ed7-490e-a217-1089a2190b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = submission('https://www.reddit.com/r/AskReddit/comments/19ewqco/what_is_the_worst_reply_to_im_leaving_you/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee432d1-1a8e-4714-9c7f-22b6f588717e",
   "metadata": {},
   "source": [
    "# 4. Word norms and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6a768-19df-4fa4-a264-dce48c6d6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "vad = pd.read_excel('vad.xlsx', index_col = 0)  #VAD norms\n",
    "sm = pd.read_excel('sensorimotor.xlsx', index_col = 0) #Sensorimotor norms\n",
    "sm = sm[['auditory', 'gustatory', 'haptic', 'interoceptive', 'olfactory',\n",
    "       'visual', 'foot_leg', 'hand_arm', 'head', 'mouth', 'torso']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5aa603-a907-4f4c-8276-a981ae27cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_s = vad.sample(500)\n",
    "\n",
    "fig = px.scatter_3d(vad_s, x='valence', y='arousal', z='dominance', hover_data = [vad_s.index])\n",
    "fig.update_traces(marker=dict(size=5,\n",
    "                              line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe2c24-11dc-4d9e-b703-0bd29fe9ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Spiders, crime, and earthquakes haunt my nightmares\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmas = [lemmatizer.lemmatize(i.lower()) for i in tokens]\n",
    "\n",
    "words = []\n",
    "emo = []\n",
    "\n",
    "for i in lemmas:\n",
    "    if i in vad.index:\n",
    "        emo.append(vad.loc[i])\n",
    "        words.append(i)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f304ab7-020b-4ffd-bd79-3a957fcd65a0",
   "metadata": {},
   "source": [
    "### 5. Document representation using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5924eca-6b4f-4901-94d3-730977b5272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "781a7241-6108-4903-96a3-97856163240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content', strip_accents = 'ascii', stop_words = 'english')\n",
    "\n",
    "D = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "\"Lazy pandas eat, sleep, and play all day without a care.\",\n",
    "\"Foxes are known for their quick movements and cunning nature.\",\n",
    "\"Unlike the active fox, the sloth is a very lazy animal.\",\n",
    "\"The quick hare was no match for the tortoise in the end.\"]\n",
    "\n",
    "v = vectorizer.fit_transform(D)\n",
    "v = v.todense().tolist()\n",
    "\n",
    "d = pd.DataFrame(\n",
    "    v,columns=vectorizer.get_feature_names_out())\n",
    "d.index = ['d1', 'd2', 'd3', 'd4', 'd5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3890646b-c2da-45f5-8596-2c58221d72a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>active</th>\n",
       "      <th>animal</th>\n",
       "      <th>brown</th>\n",
       "      <th>care</th>\n",
       "      <th>cunning</th>\n",
       "      <th>day</th>\n",
       "      <th>dog</th>\n",
       "      <th>eat</th>\n",
       "      <th>end</th>\n",
       "      <th>fox</th>\n",
       "      <th>...</th>\n",
       "      <th>match</th>\n",
       "      <th>movements</th>\n",
       "      <th>nature</th>\n",
       "      <th>pandas</th>\n",
       "      <th>play</th>\n",
       "      <th>quick</th>\n",
       "      <th>sleep</th>\n",
       "      <th>sloth</th>\n",
       "      <th>tortoise</th>\n",
       "      <th>unlike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428411</td>\n",
       "      <td>0.428411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      active    animal     brown      care   cunning       day       dog  \\\n",
       "d1  0.000000  0.000000  0.468913  0.000000  0.000000  0.000000  0.468913   \n",
       "d2  0.000000  0.000000  0.000000  0.393795  0.000000  0.393795  0.000000   \n",
       "d3  0.000000  0.000000  0.000000  0.000000  0.428411  0.000000  0.000000   \n",
       "d4  0.442832  0.442832  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "d5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         eat       end       fox  ...     match  movements    nature  \\\n",
       "d1  0.000000  0.000000  0.378316  ...  0.000000   0.000000  0.000000   \n",
       "d2  0.393795  0.000000  0.000000  ...  0.000000   0.000000  0.000000   \n",
       "d3  0.000000  0.000000  0.000000  ...  0.000000   0.428411  0.428411   \n",
       "d4  0.000000  0.000000  0.357274  ...  0.000000   0.000000  0.000000   \n",
       "d5  0.000000  0.474125  0.000000  ...  0.474125   0.000000  0.000000   \n",
       "\n",
       "      pandas      play     quick     sleep     sloth  tortoise    unlike  \n",
       "d1  0.000000  0.000000  0.314037  0.000000  0.000000  0.000000  0.000000  \n",
       "d2  0.393795  0.393795  0.000000  0.393795  0.000000  0.000000  0.000000  \n",
       "d3  0.000000  0.000000  0.286912  0.000000  0.000000  0.000000  0.000000  \n",
       "d4  0.000000  0.000000  0.000000  0.000000  0.442832  0.000000  0.442832  \n",
       "d5  0.000000  0.000000  0.317527  0.000000  0.000000  0.474125  0.000000  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc6043-4939-4c58-8d1a-6ae99f9a3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [[] for i in range(len(d))]\n",
    "\n",
    "for i in range(len(d)):\n",
    "    for j in range(len(d)):\n",
    "        distances[i].append(distance.cosine(d.iloc[i], d.iloc[j]))\n",
    "        \n",
    "dist_df = pd.DataFrame(distances, columns = d.index, index = d.index)\n",
    "\n",
    "sns.heatmap(dist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f4b99-5e3d-4985-b72c-fad36ec7fd7f",
   "metadata": {},
   "source": [
    "# 6. Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080b1f7-1485-4d24-ae7f-95748a1ddcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "with open('shake.txt', 'r') as f:\n",
    "    shake = f.read()\n",
    "\n",
    "# Get rid of newline characters and non-ascii gibberish and make everything lowercase\n",
    "shake = shake.encode('ascii', 'ignore')\n",
    "shake = shake.decode()\n",
    "shake = ' '.join(shake.splitlines())\n",
    "shake = shake.lower()\n",
    "\n",
    "shake = shake.split('.')\n",
    "shake = [i.strip() for i in shake]\n",
    "\n",
    "shake_tokens = [word_tokenize(i) for i in shake]\n",
    "\n",
    "shake_lemmas = [[] for i in range(len(shake_tokens))]\n",
    "\n",
    "for i in range(len(shake_tokens)):\n",
    "    for j in shake_tokens[i]:\n",
    "        if j not in stops and j not in punct:\n",
    "            shake_lemmas[i].append(lemmatizer.lemmatize(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82666603-35c2-4387-b2ea-767062f5e2e7",
   "metadata": {},
   "source": [
    "# 7. Linear regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef469a4-99b8-4bb4-8d85-b716fff71940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fc5e1-a044-4a9a-ab32-074eaa2ab1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_excel('/Users/jamescarney/Downloads/archive (10)/Movie_regression.xlsx')\n",
    "movie = movie.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2ed5e-8f28-4736-b075-d6b4a1318bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression() #Call the model\n",
    "\n",
    "movie = movie[movie['Genre'] == 'Thriller']\n",
    "X = movie[[i for i in movie.columns if i != 'Genre' and i != '3D_available' and i != 'Critic_rating']]\n",
    "y = movie['Critic_rating']\n",
    "\n",
    "reg = linreg.fit(X,y) #Fit the model\n",
    "\n",
    "print(reg.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1579cbb-e4cd-48a1-9c2d-736b4731ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = r_regression(X, y)\n",
    "corr_df = pd.DataFrame(correlation, index = X.columns, columns = ['correlation'])\n",
    "corr_df\n",
    "sns.barplot(y = corr_df.index, x = 'correlation', data = corr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05bb40-2c59-4199-a1a2-122538eef697",
   "metadata": {},
   "source": [
    "# 8. Logistic regression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02fbe6-681e-41ad-8109-125e5ccdf7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035e926-33fc-478e-904c-e04adc102164",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = pd.read_csv('profanity_en.csv') #opens our list of profanity words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d12a2-c55e-44a3-8592-cd55fa6e6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = [i for i in vad.index]\n",
    "good_words = random.sample(good_words, 500)\n",
    "\n",
    "good_words_ = []\n",
    "good_word_vectors = []\n",
    "\n",
    "for i in good_words:\n",
    "    try:\n",
    "        good_word_vectors.append(nlp(i).vector)\n",
    "        good_words_.append(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "bad_words = []\n",
    "bad_word_vectors = []\n",
    "\n",
    "for i in prof['text']:\n",
    "    try:\n",
    "        bad_word_vectors.append(nlp(i).vector)\n",
    "        bad_words.append(i)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c35f9-e067-42ef-b98b-5a682b1beb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity = pd.DataFrame()\n",
    "profanity['words'] = bad_words\n",
    "profanity['label'] = 1\n",
    "bad_vecs = pd.DataFrame(bad_word_vectors)\n",
    "profanity = pd.concat([profanity, bad_vecs], axis = 1)\n",
    "\n",
    "not_prof = pd.DataFrame()\n",
    "not_prof['words'] = good_words_\n",
    "not_prof['label'] = 0\n",
    "good_vecs = pd.DataFrame(good_word_vectors)\n",
    "not_prof = pd.concat([not_prof, good_vecs], axis = 1)\n",
    "\n",
    "data = pd.concat([profanity, not_prof]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f526e5-7103-4792-a2ed-a45f291976b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['words', 'label'], axis = 'columns')\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train) #This fits the model to the training data\n",
    "\n",
    "preds = clf.predict(X_test) # This outputs the model predictions on the withheld test data\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e81db-e8e6-45cb-b6b0-c81ad102cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = ['snot', 'scum', 'turd', 'piss', 'commie', 'nazi', 'bloody', 'peasant', 'bigot']\n",
    "\n",
    "tests = []\n",
    "\n",
    "for i in test_cases:\n",
    "    tests.append(clf.predict(model[i.lower()].reshape(1, -1))[0])\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['word'] = test_cases\n",
    "test_df['prediction'] = tests\n",
    "\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
